# **ローカルLLM推論環境**

このリポジトリは、ローカル環境で大規模言語モデル（LLM）を実行するためのセットアップを提供します。
モデルはHugging Face形式（`.safetensors`や`.bin`）で保存されており、Pythonスクリプトを使用して推論を行います。

## 【朗報】りもこ、Rust初心者

ちょっと脇道に逸れますが、こういう感じのことも書いとこうと思います。  
というのも、昨年（2024年）、とある用事で80年代のCで書かれたプログラムを見ていて、当時のプログラマーの人たちのあたたかなコメントアウトがとても温かく、そしておもしろかったののが印象的で、自分もそういう感じにしよーと思ったからです。  
当時は雑誌や電話、郵便でのコミュニケーションが主流だったからか、コメントアウトにもいろんなことが書いてありました。 
「わからない時に助けてもらった、今度お酒でも持っていきます！」とか、そういうことも。  
いまはネットでなんでもできますが、そういう技術者どうしのあたたかい交流とかっていう部分、いまだにありますよね。わたしは好きです。   
そこで交わされていたエンジニアどうしの人間的なやりとりが、なんか心にぐっときたっていうか。そんな感じです。  

ちなみに、いまも、ネットの海の中にはこういう古いコードが漂ってますが、読んでみるとおもろいです。

実際、Cもコーディング規約が微妙に変わってきているので、当時のコードそのままでは動きませんでした。  
他人が書いたコード、ましてやCなので、けっこうやっかいでした。  
でも、そんなコードを読み解き、現代のCコンパイラにあわせた仕様に書き換えるときにも、途中にあるユーモアと愛に溢れたコメントに、だいぶ助けられました。  

で、そんなコードを見ていて、自分もそういう感じにしてみたいなーと思ったんです。  
なので、今回は  READMEにこういうコメントを書いてみたいなーと思ったので書いてみました。

## 本当に初心者だよ？

まぁ、Rustはちょこちょこは触ってたけど、ちゃんと作るのはじめてです。  
なので、りもこはRust初心者です。**まじです**。  
・・・でも異論は認めます。たぶん。

で、今回は・・・

「Rustの実装はo3-miniさんにいっぱいアドバイスもらいながらやったよ！」  
「でも設計と方向性はりもこが決めたよ！」  
「それを形にするのにRustを勉強しながら頑張ったよ！」  
「それでできたものがこれだよ！」  
「ぜひ遊んでね！」  

って感じです。**ビバOpenAI!**

## 🚀 Rust初心者りもこの今回の学びポイント！
このプロジェクトを通して、りもこはこんなことを学んだよ！

- Rustの型システムがめちゃくちゃしっかりしてる（安心感がやばい！）
- C/C++よりメモリ管理が楽ちん！ 
- 変数の所有権（Ownership）とライフタイム（Lifetime）がとにかく便利！
- C/C++で心が折れた人にも試してほしい！！  
- `async/await` での非同期処理も便利！    
- `reqwest` でAPI通信を簡単にできるのもよき！

全体的には、

「CやC++のような低レベル言語でやっていたことが、Rustではこうやってやれるんだ！」 

ってなことがわかって、個人的には大満足でした！  
正直、C/C++やSwiftを触っていた人にはRustはとっつきやすいと思います。
まぁ、もちろん「自分でガリガリメモリ管理してるぜ！」って感じはあんまりないけど、でもこれはこれでアリ！  
そう思えるし、何より書いててめっちゃ楽しい！

*技術で遊ぶのは楽しい！*  
*それを形にするのにRustは最適かもしれん！*

そう思ったのでしたー。

---

## **特徴**

- **ローカル推論対応**
    - `.safetensors`や`.bin`形式のモデルを利用してプライバシーを確保しつつ推論可能。
- **柔軟なカスタマイズ**
    - `config.json`でモデルや推論パラメータを簡単に設定可能。
- **Hugging Face Transformers対応**
    - Transformersライブラリを使用してモデルをロード＆推論。

特に今回は、「APIの種類を判別してリクエストの形を変える」という部分がこだわりポイントです。

---

## **ファイル構成**


ファイル構成は以下の通りです。
```
.
├── LICENSE                          # ライセンス情報
├── NOTICE                           # ライセンスに関する追加情報
├── README.md                        # このREADMEファイル
├── config.json                      # 推論設定ファイル
├── configuration_qwen.py            # モデル設定スクリプト
├── cpp_kernels.py                   # カスタムCUDAカーネル（高速化用）
├── model-00001-of-00002.safetensors # safetensors形式のモデルデータ（分割1）
├── model-00002-of-00002.safetensors # safetensors形式のモデルデータ（分割2）
├── model.safetensors.index.json     # safetensorsモデルのインデックスファイル
├── modeling_qwen.py                 # モデルアーキテクチャ定義
├── pytorch_model-00001-of-00002.bin # PyTorch形式のモデルデータ（分割1）
├── pytorch_model-00002-of-00002.bin # PyTorch形式のモデルデータ（分割2）
├── pytorch_model.bin.index.json     # PyTorchモデルのインデックスファイル
├── qwen.tiktoken                    # トークナイザ関連ファイル
├── qwen_generation_utils.py         # 推論処理用ユーティリティスクリプト
├── rinna.png                        # サンプル画像またはロゴ？
├── tokenizer_config.json            # トークナイザ設定ファイル
└── cache_autogptq_cuda_*            # 自動生成されるCUDAキャッシュファイル
```


### **主要ファイル例**

- `config.json`
推論に必要な設定（モデル名、エンドポイントなど）を記述したJSONファイル。

### **モデルデータの例**
以下はRinna(nekomata-7b)を使用する場合の例です。  
Hugging Faceのモデルページからモデルデータをダウンロードしてください。  

https://huggingface.co/rinna/nekomata-7b/tree/main

ダウンロードしたモデルデータは、`models`フォルダに配置してください。   

- `model-00001-of-00002.safetensors`, `model-00002-of-00002.safetensors`
モデルデータ（safetensors形式）。複数ファイルに分割されています。
- `pytorch_model-00001-of-00002.bin`, `pytorch_model-00002-of-00002.bin`
モデルデータ（PyTorch形式）。
- `tokenizer_config.json`
トークナイザの設定ファイル。


### **スクリプト**

- `qwen_generation_utils.py`
推論処理に必要なユーティリティ関数を含むスクリプト。
- `configuration_qwen.py`, `modeling_qwen.py`
モデルの設定やアーキテクチャに関連するコード。

---

## **セットアップ**

### **1. 必要なライブラリのインストール**

以下のコマンドで必要なPythonライブラリをインストールします：

```bash
pip install torch transformers safetensors
```


### **2. モデルデータの配置**

モデルデータ（`.safetensors`または`.bin`）がリポジトリ内に存在することを確認してください。必要に応じて、Hugging Face Hubなどからダウンロードしてください。

---

## **使い方**

### **1. 設定ファイルの編集**

`config.json` を編集し、使用するモデルやパラメータを指定します。以下はサンプルです：

```json
{
  "model_name": "qwen",
  "max_tokens": 128,
  "temperature": 0.7
}
```


### **2. 推論の実行**

以下のコマンドで推論スクリプトを実行します：

```bash
python qwen_generation_utils.py --prompt "Why is the sky blue?"
```

結果が標準出力に表示されます。

---

## **カスタマイズ**

### **1. モデルの切り替え**

`config.json` の `"model_name"` を変更することで、別のモデルを使用できます。

### **2. パラメータ調整**

以下のパラメータを変更することで、生成結果をカスタマイズできます：

- `"max_tokens"`: 出力されるトークン数の最大値。
- `"temperature"`: テキスト生成時のランダム性（低いほど決定的な応答）。

---

## **注意事項**

1. モデルサイズが非常に大きいため、十分なメモリとディスク容量が必要です。
2. 推論速度はハードウェア性能（特にGPU）に依存します。GPUがない場合、推論が遅くなる可能性があります。

---

## **ライセンス**

このプロジェクトは、それぞれのライセンス条件に従って利用できます。
